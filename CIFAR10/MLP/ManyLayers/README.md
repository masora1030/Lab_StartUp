## 5層のMLPを構成した。
## (Affine relu (dropout) | Affine relu (dropout) | Affine relu | Affine relu | Affine (Softmax))
- 隠れ層のユニット数は、ピラミッド型にしても入力データと同じ数にしてもあまり結果は変わらなかったが、入力データと同じ数の方が若干学習率が高かった。
- 隠れ層のユニット数を1024(32*32)より大きくするとうまく学習が行かない。
- DropOutは1,2層でやるのが最も学習率が高かった。1層のみでも良い。2層のみやDropOutなしだと過学習しがち。
- 学習時間や学習率は、より多層にしてもあまり変わらなかった。(むしろ過学習傾向に)
