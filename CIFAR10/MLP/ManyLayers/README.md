## CIFAR10のデータセットに対して、5層のMLPを構成した。
### (Affine relu (dropout) | Affine relu (dropout) | Affine relu | Affine relu | Affine (Softmax))
- 隠れ層のユニット数は、ピラミッド型にしても入力データと同じ数にしてもあまり結果は変わらなかったが、入力データと同じ数の方が若干正答率が高かった。
- 隠れ層のユニット数を1024(32*32)より大きくするとうまく学習が行かない。
- DropOutは1,2層でやるのが最も正答率が高かった。1層のみでも良い。2層のみやDropOutなしだと過学習しがち。
- 学習時間や正答率は、より多層にしてもさほど変わらなかった。(3秒ほど遅くなり、ほんのわずかに学習率は上昇した。)
- Epochを増やしても正答率に大きな変化はなかった。(TestCaseに過学習していた。)
- データ正規化を加えると、Testの正答率が2%ほど上昇し、学習曲線に変化があった。学習時間は1.5倍ほどになった。
