0it [00:00, ?it/s]  0%|          | 0/170498071 [00:00<?, ?it/s]  0%|          | 8192/170498071 [00:01<58:50, 48286.75it/s]  0%|          | 40960/170498071 [00:01<45:39, 62227.66it/s]  0%|          | 90112/170498071 [00:01<34:55, 81302.09it/s]  0%|          | 221184/170498071 [00:01<25:33, 111037.74it/s]  0%|          | 450560/170498071 [00:01<18:30, 153187.01it/s]  1%|          | 909312/170498071 [00:01<13:14, 213521.14it/s]  1%|          | 1826816/170498071 [00:02<09:22, 299918.95it/s]  2%|▏         | 3678208/170498071 [00:02<06:34, 423339.50it/s]  4%|▍         | 6807552/170498071 [00:02<04:33, 598762.95it/s]  6%|▌         | 9920512/170498071 [00:02<03:10, 843347.76it/s]  8%|▊         | 13033472/170498071 [00:02<02:13, 1181168.64it/s]  9%|▉         | 16146432/170498071 [00:02<01:34, 1640638.15it/s] 11%|█▏        | 19210240/170498071 [00:03<01:07, 2256191.85it/s] 13%|█▎        | 22257664/170498071 [00:03<00:48, 3055779.19it/s] 15%|█▍        | 25141248/170498071 [00:03<00:36, 4037633.50it/s] 16%|█▋        | 28123136/170498071 [00:03<00:27, 5240346.47it/s] 18%|█▊        | 31186944/170498071 [00:03<00:20, 6646159.93it/s] 20%|██        | 34283520/170498071 [00:03<00:16, 8478962.07it/s] 21%|██        | 36184064/170498071 [00:04<00:13, 10155041.46it/s] 22%|██▏       | 38084608/170498071 [00:04<00:11, 11052104.16it/s] 24%|██▎       | 40378368/170498071 [00:04<00:10, 12915979.74it/s] 25%|██▍       | 42262528/170498071 [00:04<00:09, 13990721.09it/s] 26%|██▌       | 44089344/170498071 [00:04<00:08, 14255588.66it/s] 27%|██▋       | 46538752/170498071 [00:04<00:07, 15875104.28it/s] 28%|██▊       | 48406528/170498071 [00:04<00:07, 16227652.97it/s] 29%|██▉       | 50225152/170498071 [00:04<00:07, 15779580.22it/s] 31%|███       | 52338688/170498071 [00:04<00:07, 16584145.02it/s] 32%|███▏      | 54550528/170498071 [00:05<00:06, 17547261.59it/s] 33%|███▎      | 56401920/170498071 [00:05<00:06, 16656278.48it/s] 34%|███▍      | 58335232/170498071 [00:05<00:06, 16692603.06it/s] 36%|███▌      | 60547072/170498071 [00:05<00:06, 17795258.05it/s] 37%|███▋      | 62382080/170498071 [00:05<00:06, 16782937.59it/s] 38%|███▊      | 64446464/170498071 [00:05<00:06, 17117568.78it/s] 39%|███▉      | 66461696/170498071 [00:05<00:05, 17598397.56it/s] 40%|████      | 68255744/170498071 [00:05<00:06, 16277941.82it/s] 41%|████▏     | 70639616/170498071 [00:06<00:05, 17675315.62it/s] 43%|████▎     | 72589312/170498071 [00:06<00:05, 17866757.20it/s] 44%|████▎     | 74424320/170498071 [00:06<00:05, 16395989.43it/s] 45%|████▌     | 76734464/170498071 [00:06<00:05, 17801050.50it/s] 46%|████▌     | 78700544/170498071 [00:06<00:05, 17982016.13it/s] 47%|████▋     | 80551936/170498071 [00:06<00:05, 16423625.27it/s] 48%|████▊     | 82468864/170498071 [00:06<00:05, 16846954.54it/s] 50%|████▉     | 84860928/170498071 [00:06<00:04, 18407584.30it/s] 51%|█████     | 86777856/170498071 [00:06<00:04, 16770215.84it/s] 52%|█████▏    | 88580096/170498071 [00:07<00:04, 16891102.36it/s] 53%|█████▎    | 90611712/170498071 [00:07<00:04, 17789858.32it/s] 54%|█████▍    | 92446720/170498071 [00:07<00:04, 16888119.44it/s] 56%|█████▌    | 94691328/170498071 [00:07<00:04, 17170821.29it/s] 57%|█████▋    | 96657408/170498071 [00:07<00:04, 17347498.42it/s] 58%|█████▊    | 98418688/170498071 [00:07<00:04, 16810777.59it/s] 59%|█████▉    | 100818944/170498071 [00:07<00:03, 17596826.63it/s] 60%|██████    | 102834176/170498071 [00:07<00:03, 17807833.12it/s] 61%|██████▏   | 104636416/170498071 [00:07<00:03, 17119566.03it/s] 63%|██████▎   | 106848256/170498071 [00:08<00:03, 17538369.91it/s] 64%|██████▍   | 109027328/170498071 [00:08<00:03, 18168953.73it/s] 65%|██████▌   | 110862336/170498071 [00:08<00:03, 16954458.50it/s] 66%|██████▌   | 112877568/170498071 [00:08<00:03, 17191885.28it/s] 68%|██████▊   | 115171328/170498071 [00:08<00:03, 18355544.58it/s] 69%|██████▊   | 117047296/170498071 [00:08<00:03, 17172875.49it/s] 70%|██████▉   | 119037952/170498071 [00:08<00:02, 17281919.73it/s] 71%|███████   | 120840192/170498071 [00:08<00:02, 17491198.78it/s] 72%|███████▏  | 122617856/170498071 [00:09<00:02, 16461175.82it/s] 73%|███████▎  | 125132800/170498071 [00:09<00:02, 17571834.80it/s] 74%|███████▍  | 126935040/170498071 [00:09<00:02, 17652271.78it/s] 76%|███████▌  | 128729088/170498071 [00:09<00:02, 16273399.90it/s] 77%|███████▋  | 131342336/170498071 [00:09<00:02, 17851504.25it/s] 78%|███████▊  | 133210112/170498071 [00:09<00:02, 17770121.84it/s] 79%|███████▉  | 135045120/170498071 [00:09<00:02, 16509050.54it/s] 81%|████████  | 137420800/170498071 [00:09<00:01, 17773562.71it/s] 82%|████████▏ | 139264000/170498071 [00:09<00:01, 17641299.00it/s] 83%|████████▎ | 141074432/170498071 [00:10<00:01, 17099044.73it/s] 84%|████████▍ | 143155200/170498071 [00:10<00:01, 18061947.71it/s] 85%|████████▌ | 145006592/170498071 [00:10<00:01, 16324542.69it/s] 86%|████████▌ | 146874368/170498071 [00:10<00:01, 16704340.40it/s] 88%|████████▊ | 149299200/170498071 [00:10<00:01, 18417307.49it/s] 89%|████████▊ | 151232512/170498071 [00:10<00:01, 16883503.13it/s] 90%|████████▉ | 153010176/170498071 [00:10<00:01, 16624281.86it/s] 91%|█████████ | 155443200/170498071 [00:10<00:00, 18338961.59it/s] 92%|█████████▏| 157376512/170498071 [00:10<00:00, 17002482.92it/s] 93%|█████████▎| 159170560/170498071 [00:11<00:00, 16867593.56it/s] 94%|█████████▍| 160964608/170498071 [00:11<00:00, 16991063.08it/s] 96%|█████████▌| 162963456/170498071 [00:11<00:00, 17659007.73it/s] 97%|█████████▋| 165175296/170498071 [00:11<00:00, 18699735.29it/s] 98%|█████████▊| 167092224/170498071 [00:11<00:00, 16371444.48it/s] 99%|█████████▉| 169246720/170498071 [00:11<00:00, 17641180.05it/s]Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_cifar10/cifar-10-python.tar.gz
Extracting ./data_cifar10/cifar-10-python.tar.gz to ./data_cifar10/
Files already downloaded and verified
train_dataset =  40000
valid_dataset =  10000
test_dataset =  10000
MyManyMLP training start ...
Epoch [1/50], Loss: 0.0288, val_loss: 0.0247, val_acc: 0.4403
Epoch [2/50], Loss: 0.0250, val_loss: 0.0239, val_acc: 0.4583
170500096it [00:30, 17641180.05it/s]                               Epoch [3/50], Loss: 0.0231, val_loss: 0.0225, val_acc: 0.4973
Epoch [4/50], Loss: 0.0218, val_loss: 0.0219, val_acc: 0.5155
Epoch [5/50], Loss: 0.0206, val_loss: 0.0217, val_acc: 0.5126
Epoch [6/50], Loss: 0.0197, val_loss: 0.0216, val_acc: 0.5232
Epoch [7/50], Loss: 0.0188, val_loss: 0.0210, val_acc: 0.5354
Epoch [8/50], Loss: 0.0180, val_loss: 0.0209, val_acc: 0.5386
Epoch [9/50], Loss: 0.0173, val_loss: 0.0211, val_acc: 0.5396
Epoch [10/50], Loss: 0.0165, val_loss: 0.0207, val_acc: 0.5438
Epoch [11/50], Loss: 0.0160, val_loss: 0.0207, val_acc: 0.5526
Epoch [12/50], Loss: 0.0154, val_loss: 0.0207, val_acc: 0.5515
Epoch [13/50], Loss: 0.0148, val_loss: 0.0207, val_acc: 0.5546
Epoch [14/50], Loss: 0.0142, val_loss: 0.0207, val_acc: 0.5613
Epoch [15/50], Loss: 0.0135, val_loss: 0.0210, val_acc: 0.5636
Epoch [16/50], Loss: 0.0131, val_loss: 0.0208, val_acc: 0.5593
Epoch [17/50], Loss: 0.0126, val_loss: 0.0211, val_acc: 0.5612
Epoch [18/50], Loss: 0.0121, val_loss: 0.0215, val_acc: 0.5596
Epoch [19/50], Loss: 0.0117, val_loss: 0.0220, val_acc: 0.5524
Epoch [20/50], Loss: 0.0113, val_loss: 0.0217, val_acc: 0.5614
Epoch [21/50], Loss: 0.0109, val_loss: 0.0218, val_acc: 0.5637
Epoch [22/50], Loss: 0.0105, val_loss: 0.0223, val_acc: 0.5616
Epoch [23/50], Loss: 0.0102, val_loss: 0.0227, val_acc: 0.5546
Epoch [24/50], Loss: 0.0099, val_loss: 0.0225, val_acc: 0.5568
Epoch [25/50], Loss: 0.0095, val_loss: 0.0232, val_acc: 0.5558
Epoch [26/50], Loss: 0.0092, val_loss: 0.0229, val_acc: 0.5589
Epoch [27/50], Loss: 0.0090, val_loss: 0.0234, val_acc: 0.5498
Epoch [28/50], Loss: 0.0087, val_loss: 0.0236, val_acc: 0.5523
Epoch [29/50], Loss: 0.0085, val_loss: 0.0234, val_acc: 0.5587
Epoch [30/50], Loss: 0.0084, val_loss: 0.0234, val_acc: 0.5589
Epoch [31/50], Loss: 0.0080, val_loss: 0.0240, val_acc: 0.5621
Epoch [32/50], Loss: 0.0079, val_loss: 0.0247, val_acc: 0.5507
Epoch [33/50], Loss: 0.0077, val_loss: 0.0244, val_acc: 0.5575
Epoch [34/50], Loss: 0.0077, val_loss: 0.0241, val_acc: 0.5488
Epoch [35/50], Loss: 0.0075, val_loss: 0.0246, val_acc: 0.5564
Epoch [36/50], Loss: 0.0073, val_loss: 0.0244, val_acc: 0.5601
Epoch [37/50], Loss: 0.0071, val_loss: 0.0245, val_acc: 0.5588
Epoch [38/50], Loss: 0.0071, val_loss: 0.0251, val_acc: 0.5539
Epoch [39/50], Loss: 0.0069, val_loss: 0.0253, val_acc: 0.5559
Epoch [40/50], Loss: 0.0068, val_loss: 0.0253, val_acc: 0.5576
Epoch [41/50], Loss: 0.0068, val_loss: 0.0257, val_acc: 0.5511
Epoch [42/50], Loss: 0.0068, val_loss: 0.0250, val_acc: 0.5575
Epoch [43/50], Loss: 0.0065, val_loss: 0.0251, val_acc: 0.5565
Epoch [44/50], Loss: 0.0066, val_loss: 0.0252, val_acc: 0.5599
Epoch [45/50], Loss: 0.0065, val_loss: 0.0258, val_acc: 0.5498
Epoch [46/50], Loss: 0.0064, val_loss: 0.0258, val_acc: 0.5489
Epoch [47/50], Loss: 0.0064, val_loss: 0.0260, val_acc: 0.5566
Epoch [48/50], Loss: 0.0064, val_loss: 0.0253, val_acc: 0.5590
Epoch [49/50], Loss: 0.0062, val_loss: 0.0260, val_acc: 0.5515
Epoch [50/50], Loss: 0.0061, val_loss: 0.0258, val_acc: 0.5529
test_accuracy: 55.7 %
learning time: 153.495 [sec]
MyFewMLP training start ...
Epoch [1/50], Loss: 0.0302, val_loss: 0.0265, val_acc: 0.4226
Epoch [2/50], Loss: 0.0260, val_loss: 0.0241, val_acc: 0.4672
Epoch [3/50], Loss: 0.0242, val_loss: 0.0233, val_acc: 0.4805
Epoch [4/50], Loss: 0.0229, val_loss: 0.0225, val_acc: 0.5041
Epoch [5/50], Loss: 0.0217, val_loss: 0.0220, val_acc: 0.5072
Epoch [6/50], Loss: 0.0208, val_loss: 0.0214, val_acc: 0.5181
Epoch [7/50], Loss: 0.0199, val_loss: 0.0215, val_acc: 0.5252
Epoch [8/50], Loss: 0.0190, val_loss: 0.0206, val_acc: 0.5429
Epoch [9/50], Loss: 0.0183, val_loss: 0.0207, val_acc: 0.5391
Epoch [10/50], Loss: 0.0176, val_loss: 0.0205, val_acc: 0.5374
Epoch [11/50], Loss: 0.0170, val_loss: 0.0204, val_acc: 0.5497
Epoch [12/50], Loss: 0.0164, val_loss: 0.0203, val_acc: 0.5458
Epoch [13/50], Loss: 0.0159, val_loss: 0.0201, val_acc: 0.5560
Epoch [14/50], Loss: 0.0153, val_loss: 0.0202, val_acc: 0.5605
Epoch [15/50], Loss: 0.0149, val_loss: 0.0200, val_acc: 0.5634
Epoch [16/50], Loss: 0.0143, val_loss: 0.0202, val_acc: 0.5549
Epoch [17/50], Loss: 0.0139, val_loss: 0.0200, val_acc: 0.5585
Epoch [18/50], Loss: 0.0135, val_loss: 0.0202, val_acc: 0.5610
Epoch [19/50], Loss: 0.0130, val_loss: 0.0201, val_acc: 0.5635
Epoch [20/50], Loss: 0.0127, val_loss: 0.0204, val_acc: 0.5621
Epoch [21/50], Loss: 0.0123, val_loss: 0.0206, val_acc: 0.5528
Epoch [22/50], Loss: 0.0119, val_loss: 0.0204, val_acc: 0.5599
Epoch [23/50], Loss: 0.0115, val_loss: 0.0208, val_acc: 0.5588
Epoch [24/50], Loss: 0.0111, val_loss: 0.0206, val_acc: 0.5602
Epoch [25/50], Loss: 0.0109, val_loss: 0.0208, val_acc: 0.5612
Epoch [26/50], Loss: 0.0105, val_loss: 0.0212, val_acc: 0.5645
Epoch [27/50], Loss: 0.0103, val_loss: 0.0209, val_acc: 0.5589
Epoch [28/50], Loss: 0.0100, val_loss: 0.0212, val_acc: 0.5651
Epoch [29/50], Loss: 0.0097, val_loss: 0.0215, val_acc: 0.5614
Epoch [30/50], Loss: 0.0096, val_loss: 0.0215, val_acc: 0.5604
Epoch [31/50], Loss: 0.0094, val_loss: 0.0215, val_acc: 0.5585
Epoch [32/50], Loss: 0.0091, val_loss: 0.0217, val_acc: 0.5601
Epoch [33/50], Loss: 0.0090, val_loss: 0.0213, val_acc: 0.5682
Epoch [34/50], Loss: 0.0088, val_loss: 0.0218, val_acc: 0.5603
Epoch [35/50], Loss: 0.0086, val_loss: 0.0219, val_acc: 0.5585
Epoch [36/50], Loss: 0.0085, val_loss: 0.0217, val_acc: 0.5672
Epoch [37/50], Loss: 0.0083, val_loss: 0.0218, val_acc: 0.5685
Epoch [38/50], Loss: 0.0082, val_loss: 0.0225, val_acc: 0.5551
Epoch [39/50], Loss: 0.0080, val_loss: 0.0225, val_acc: 0.5553
Epoch [40/50], Loss: 0.0078, val_loss: 0.0224, val_acc: 0.5570
Epoch [41/50], Loss: 0.0079, val_loss: 0.0226, val_acc: 0.5595
Epoch [42/50], Loss: 0.0076, val_loss: 0.0226, val_acc: 0.5641
Epoch [43/50], Loss: 0.0075, val_loss: 0.0222, val_acc: 0.5644
Epoch [44/50], Loss: 0.0074, val_loss: 0.0232, val_acc: 0.5579
Epoch [45/50], Loss: 0.0072, val_loss: 0.0234, val_acc: 0.5517
Epoch [46/50], Loss: 0.0073, val_loss: 0.0228, val_acc: 0.5636
Epoch [47/50], Loss: 0.0070, val_loss: 0.0229, val_acc: 0.5654
Epoch [48/50], Loss: 0.0071, val_loss: 0.0228, val_acc: 0.5567
Epoch [49/50], Loss: 0.0069, val_loss: 0.0230, val_acc: 0.5627
Epoch [50/50], Loss: 0.0069, val_loss: 0.0231, val_acc: 0.5580
test_accuracy: 55.44 %
learning time: 153.488 [sec]

--------------------------------------------------------------

network: MyManyMLP  |  test_accuracy: 55.7 %  |  learning time: 153.495 [sec]
network: MyFewMLP  |  test_accuracy: 55.44 %  |  learning time: 153.488 [sec]
170500096it [05:31, 514328.67it/s]  
