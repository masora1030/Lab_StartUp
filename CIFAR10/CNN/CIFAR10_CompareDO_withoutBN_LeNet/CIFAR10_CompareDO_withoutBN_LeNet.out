0it [00:00, ?it/s]
0it [00:00, ?it/s][A
  0%|          | 0/170498071 [00:00<?, ?it/s][A
  0%|          | 16384/170498071 [00:00<29:49, 95248.46it/s][A
  0%|          | 49152/170498071 [00:00<25:26, 111661.72it/s][A
  0%|          | 106496/170498071 [00:00<20:21, 139479.09it/s][A
  0%|          | 221184/170498071 [00:01<15:31, 182874.62it/s][A
  0%|          | 450560/170498071 [00:01<11:29, 246705.99it/s][A
  1%|          | 917504/170498071 [00:01<08:20, 339159.95it/s][A
  1%|          | 1712128/170498071 [00:01<05:54, 475678.44it/s][A
  1%|          | 2105344/170498071 [00:01<04:20, 645871.84it/s][A
  2%|▏         | 3530752/170498071 [00:01<03:04, 905096.97it/s][A
  3%|▎         | 4685824/170498071 [00:01<02:12, 1250744.30it/s][A
  4%|▍         | 6545408/170498071 [00:01<01:35, 1718697.52it/s][A
  5%|▌         | 8617984/170498071 [00:02<01:08, 2370839.03it/s][A
  6%|▌         | 9920512/170498071 [00:02<00:51, 3132480.32it/s][A
  7%|▋         | 11968512/170498071 [00:02<00:37, 4198130.25it/s][A
  8%|▊         | 13484032/170498071 [00:02<00:29, 5257652.71it/s][A
  9%|▉         | 15179776/170498071 [00:02<00:24, 6454040.44it/s][A
 10%|█         | 17268736/170498071 [00:02<00:18, 8134002.26it/s][A
 11%|█         | 18890752/170498071 [00:02<00:16, 9364707.01it/s][A
 12%|█▏        | 20840448/170498071 [00:02<00:14, 10316575.14it/s][A
 14%|█▎        | 23027712/170498071 [00:02<00:12, 12258463.05it/s][A
 15%|█▍        | 24739840/170498071 [00:03<00:11, 13097069.43it/s][A
 16%|█▌        | 26574848/170498071 [00:03<00:11, 12885335.50it/s][A
 17%|█▋        | 28532736/170498071 [00:03<00:09, 14357338.89it/s][A
 18%|█▊        | 30195712/170498071 [00:03<00:09, 14589371.89it/s][A
 19%|█▉        | 32210944/170498071 [00:03<00:08, 15898063.19it/s][A
 20%|█▉        | 33947648/170498071 [00:03<00:09, 14391912.66it/s][A
 21%|██        | 35733504/170498071 [00:03<00:08, 15253996.37it/s][A
 22%|██▏       | 37371904/170498071 [00:03<00:08, 15497264.41it/s][A
 23%|██▎       | 39378944/170498071 [00:03<00:08, 15001137.95it/s][A
 24%|██▍       | 41082880/170498071 [00:04<00:08, 15555276.58it/s][A
 25%|██▌       | 43089920/170498071 [00:04<00:07, 16680151.94it/s][A
 26%|██▋       | 44818432/170498071 [00:04<00:08, 15403871.17it/s][A
 27%|██▋       | 46415872/170498071 [00:04<00:11, 10602894.21it/s][A
 28%|██▊       | 48513024/170498071 [00:04<00:11, 10677530.36it/s][A
 30%|██▉       | 50683904/170498071 [00:04<00:10, 11153148.29it/s][A
 32%|███▏      | 53821440/170498071 [00:05<00:09, 12624275.63it/s][A
 33%|███▎      | 56934400/170498071 [00:05<00:08, 13106330.16it/s][A
 35%|███▌      | 59826176/170498071 [00:05<00:08, 13759623.97it/s][A
 37%|███▋      | 62783488/170498071 [00:05<00:07, 14398540.81it/s][A
 39%|███▊      | 65740800/170498071 [00:05<00:07, 14854848.88it/s][A
 40%|████      | 68632576/170498071 [00:06<00:06, 15072115.65it/s][A
 42%|████▏     | 71458816/170498071 [00:06<00:06, 15177695.22it/s][A
 44%|████▎     | 74285056/170498071 [00:06<00:06, 15280555.70it/s][A
 45%|████▌     | 77045760/170498071 [00:06<00:06, 15095918.01it/s][A
 47%|████▋     | 79929344/170498071 [00:06<00:05, 15397920.28it/s][A
 49%|████▊     | 82821120/170498071 [00:06<00:05, 15584860.60it/s][A
 50%|█████     | 85778432/170498071 [00:07<00:05, 15624933.27it/s][A
 52%|█████▏    | 88670208/170498071 [00:07<00:05, 15740389.04it/s][A
 54%|█████▎    | 91627520/170498071 [00:07<00:05, 15746179.09it/s][A
 55%|█████▌    | 94511104/170498071 [00:07<00:04, 15734577.70it/s][A
 57%|█████▋    | 97402880/170498071 [00:07<00:04, 15647641.79it/s][A
 59%|█████▉    | 100294656/170498071 [00:08<00:04, 15809495.21it/s][A
 60%|██████    | 103055360/170498071 [00:08<00:04, 15614178.71it/s][A
 62%|██████▏   | 106012672/170498071 [00:08<00:04, 15627921.44it/s][A
 64%|██████▍   | 108904448/170498071 [00:08<00:03, 15482251.91it/s][A
 66%|██████▌   | 111853568/170498071 [00:08<00:03, 15512246.25it/s][A
 67%|██████▋   | 114745344/170498071 [00:09<00:03, 15545483.96it/s][A
 69%|██████▉   | 117547008/170498071 [00:09<00:04, 12690788.90it/s][A
 70%|██████▉   | 118906880/170498071 [00:09<00:04, 12830045.02it/s][A
 71%|███████   | 120659968/170498071 [00:09<00:04, 11756143.55it/s][A
 72%|███████▏  | 123518976/170498071 [00:09<00:03, 12777892.43it/s][A
 74%|███████▍  | 126058496/170498071 [00:09<00:02, 15005227.91it/s][A
 75%|███████▍  | 127762432/170498071 [00:10<00:03, 14231501.42it/s][A
 76%|███████▌  | 129335296/170498071 [00:10<00:03, 13372994.85it/s][A
 77%|███████▋  | 132063232/170498071 [00:10<00:02, 14002308.28it/s][A
 79%|███████▉  | 135036928/170498071 [00:10<00:02, 14866162.94it/s][A
 81%|████████  | 137551872/170498071 [00:10<00:01, 16912553.32it/s][A
 82%|████████▏ | 139395072/170498071 [00:10<00:02, 14581527.88it/s][A
 83%|████████▎ | 141156352/170498071 [00:10<00:02, 12995337.21it/s][A
 84%|████████▍ | 144064512/170498071 [00:11<00:01, 13609001.74it/s][A
 86%|████████▌ | 146464768/170498071 [00:11<00:01, 15639262.45it/s][A
 87%|████████▋ | 148217856/170498071 [00:11<00:01, 15133564.77it/s][A
 88%|████████▊ | 150241280/170498071 [00:11<00:01, 15508218.49it/s][A
 89%|████████▉ | 151887872/170498071 [00:11<00:01, 14124071.34it/s][A
 90%|████████▉ | 153395200/170498071 [00:11<00:01, 13200500.66it/s][A
 92%|█████████▏| 156033024/170498071 [00:11<00:01, 13378764.60it/s][A
 93%|█████████▎| 157908992/170498071 [00:12<00:00, 14562129.46it/s][A
 94%|█████████▎| 159440896/170498071 [00:12<00:00, 14045410.79it/s][A
 95%|█████████▍| 161144832/170498071 [00:12<00:00, 14799530.35it/s][A
 95%|█████████▌| 162676736/170498071 [00:12<00:00, 14602125.51it/s][A
 96%|█████████▋| 164347904/170498071 [00:12<00:00, 15093809.82it/s][A
 97%|█████████▋| 165888000/170498071 [00:12<00:00, 14614916.58it/s][A
 98%|█████████▊| 167731200/170498071 [00:12<00:00, 14349613.61it/s][A
100%|█████████▉| 169672704/170498071 [00:12<00:00, 15557848.79it/s][ADownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_cifar10/cifar-10-python.tar.gz
Failed download. Trying https -> http instead. Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_cifar10/cifar-10-python.tar.gz
Extracting ./data_cifar10/cifar-10-python.tar.gz to ./data_cifar10/
Files already downloaded and verified
train_dataset =  40000
valid_dataset =  10000
test_dataset =  10000
Default(NoDropOut) training start ...

170500096it [00:29, 15557848.79it/s]                               [AEpoch [1/50], Loss: 0.0353, val_loss: 0.0325, val_acc: 0.2278
Epoch [2/50], Loss: 0.0284, val_loss: 0.0260, val_acc: 0.3842
Epoch [3/50], Loss: 0.0243, val_loss: 0.0232, val_acc: 0.4521
Epoch [4/50], Loss: 0.0223, val_loss: 0.0218, val_acc: 0.4963
Epoch [5/50], Loss: 0.0211, val_loss: 0.0203, val_acc: 0.5352
Epoch [6/50], Loss: 0.0198, val_loss: 0.0201, val_acc: 0.5461
Epoch [7/50], Loss: 0.0188, val_loss: 0.0192, val_acc: 0.5672
Epoch [8/50], Loss: 0.0180, val_loss: 0.0191, val_acc: 0.5748
Epoch [9/50], Loss: 0.0172, val_loss: 0.0185, val_acc: 0.5901
Epoch [10/50], Loss: 0.0166, val_loss: 0.0185, val_acc: 0.5937
Epoch [11/50], Loss: 0.0161, val_loss: 0.0183, val_acc: 0.5948
Epoch [12/50], Loss: 0.0155, val_loss: 0.0187, val_acc: 0.5944
Epoch [13/50], Loss: 0.0149, val_loss: 0.0187, val_acc: 0.5945
Epoch [14/50], Loss: 0.0144, val_loss: 0.0186, val_acc: 0.5947
Epoch [15/50], Loss: 0.0140, val_loss: 0.0184, val_acc: 0.6032
Epoch [16/50], Loss: 0.0136, val_loss: 0.0184, val_acc: 0.6070
Epoch [17/50], Loss: 0.0131, val_loss: 0.0184, val_acc: 0.6026
Epoch [18/50], Loss: 0.0128, val_loss: 0.0197, val_acc: 0.5915
Epoch [19/50], Loss: 0.0125, val_loss: 0.0195, val_acc: 0.6057
Epoch [20/50], Loss: 0.0120, val_loss: 0.0202, val_acc: 0.5991
Epoch [21/50], Loss: 0.0118, val_loss: 0.0192, val_acc: 0.5949
Epoch [22/50], Loss: 0.0115, val_loss: 0.0204, val_acc: 0.5950
Epoch [23/50], Loss: 0.0109, val_loss: 0.0204, val_acc: 0.5906
Epoch [24/50], Loss: 0.0108, val_loss: 0.0204, val_acc: 0.6019
Epoch [25/50], Loss: 0.0106, val_loss: 0.0203, val_acc: 0.5917
Epoch [26/50], Loss: 0.0104, val_loss: 0.0209, val_acc: 0.5890
Epoch [27/50], Loss: 0.0101, val_loss: 0.0225, val_acc: 0.5851
Epoch [28/50], Loss: 0.0099, val_loss: 0.0232, val_acc: 0.5668
Epoch [29/50], Loss: 0.0097, val_loss: 0.0216, val_acc: 0.5873
Epoch [30/50], Loss: 0.0096, val_loss: 0.0214, val_acc: 0.5936
Epoch [31/50], Loss: 0.0093, val_loss: 0.0236, val_acc: 0.5828
Epoch [32/50], Loss: 0.0092, val_loss: 0.0232, val_acc: 0.5793
Epoch [33/50], Loss: 0.0092, val_loss: 0.0243, val_acc: 0.5813
Epoch [34/50], Loss: 0.0089, val_loss: 0.0235, val_acc: 0.5866
Epoch [35/50], Loss: 0.0087, val_loss: 0.0238, val_acc: 0.5834
Epoch [36/50], Loss: 0.0083, val_loss: 0.0241, val_acc: 0.5836
Epoch [37/50], Loss: 0.0085, val_loss: 0.0242, val_acc: 0.5738
Epoch [38/50], Loss: 0.0081, val_loss: 0.0256, val_acc: 0.5703
Epoch [39/50], Loss: 0.0084, val_loss: 0.0244, val_acc: 0.5899
Epoch [40/50], Loss: 0.0081, val_loss: 0.0246, val_acc: 0.5812
Epoch [41/50], Loss: 0.0079, val_loss: 0.0257, val_acc: 0.5812
Epoch [42/50], Loss: 0.0077, val_loss: 0.0266, val_acc: 0.5675
Epoch [43/50], Loss: 0.0079, val_loss: 0.0258, val_acc: 0.5741
Epoch [44/50], Loss: 0.0078, val_loss: 0.0255, val_acc: 0.5706
Epoch [45/50], Loss: 0.0076, val_loss: 0.0259, val_acc: 0.5740
Epoch [46/50], Loss: 0.0074, val_loss: 0.0262, val_acc: 0.5802
Epoch [47/50], Loss: 0.0075, val_loss: 0.0267, val_acc: 0.5637
Epoch [48/50], Loss: 0.0073, val_loss: 0.0267, val_acc: 0.5757
Epoch [49/50], Loss: 0.0073, val_loss: 0.0275, val_acc: 0.5793
Epoch [50/50], Loss: 0.0070, val_loss: 0.0277, val_acc: 0.5728
test_accuracy: 55.55 %
learning time: 116.382 [sec]
1DropOut training start ...
Epoch [1/50], Loss: 0.0342, val_loss: 0.0311, val_acc: 0.2824
Epoch [2/50], Loss: 0.0292, val_loss: 0.0261, val_acc: 0.3929
Epoch [3/50], Loss: 0.0260, val_loss: 0.0251, val_acc: 0.4307
Epoch [4/50], Loss: 0.0248, val_loss: 0.0233, val_acc: 0.4787
Epoch [5/50], Loss: 0.0236, val_loss: 0.0234, val_acc: 0.4770
Epoch [6/50], Loss: 0.0228, val_loss: 0.0212, val_acc: 0.5192
Epoch [7/50], Loss: 0.0220, val_loss: 0.0209, val_acc: 0.5273
Epoch [8/50], Loss: 0.0216, val_loss: 0.0212, val_acc: 0.5208
Epoch [9/50], Loss: 0.0211, val_loss: 0.0205, val_acc: 0.5329
Epoch [10/50], Loss: 0.0207, val_loss: 0.0196, val_acc: 0.5602
Epoch [11/50], Loss: 0.0204, val_loss: 0.0197, val_acc: 0.5618
Epoch [12/50], Loss: 0.0200, val_loss: 0.0194, val_acc: 0.5655
Epoch [13/50], Loss: 0.0198, val_loss: 0.0187, val_acc: 0.5808
Epoch [14/50], Loss: 0.0195, val_loss: 0.0200, val_acc: 0.5574
Epoch [15/50], Loss: 0.0192, val_loss: 0.0185, val_acc: 0.5854
Epoch [16/50], Loss: 0.0189, val_loss: 0.0184, val_acc: 0.5901
Epoch [17/50], Loss: 0.0187, val_loss: 0.0190, val_acc: 0.5697
Epoch [18/50], Loss: 0.0184, val_loss: 0.0189, val_acc: 0.5728
Epoch [19/50], Loss: 0.0184, val_loss: 0.0201, val_acc: 0.5464
Epoch [20/50], Loss: 0.0182, val_loss: 0.0184, val_acc: 0.5876
Epoch [21/50], Loss: 0.0181, val_loss: 0.0187, val_acc: 0.5816
Epoch [22/50], Loss: 0.0180, val_loss: 0.0180, val_acc: 0.5943
Epoch [23/50], Loss: 0.0179, val_loss: 0.0191, val_acc: 0.5727
Epoch [24/50], Loss: 0.0177, val_loss: 0.0186, val_acc: 0.5854
Epoch [25/50], Loss: 0.0175, val_loss: 0.0181, val_acc: 0.5926
Epoch [26/50], Loss: 0.0173, val_loss: 0.0177, val_acc: 0.6046
Epoch [27/50], Loss: 0.0174, val_loss: 0.0187, val_acc: 0.5840
Epoch [28/50], Loss: 0.0172, val_loss: 0.0175, val_acc: 0.6133
Epoch [29/50], Loss: 0.0171, val_loss: 0.0182, val_acc: 0.5926
Epoch [30/50], Loss: 0.0169, val_loss: 0.0175, val_acc: 0.6092
Epoch [31/50], Loss: 0.0170, val_loss: 0.0185, val_acc: 0.5857
Epoch [32/50], Loss: 0.0168, val_loss: 0.0177, val_acc: 0.6015
Epoch [33/50], Loss: 0.0168, val_loss: 0.0174, val_acc: 0.6120
Epoch [34/50], Loss: 0.0166, val_loss: 0.0171, val_acc: 0.6223
Epoch [35/50], Loss: 0.0167, val_loss: 0.0178, val_acc: 0.6029
Epoch [36/50], Loss: 0.0166, val_loss: 0.0173, val_acc: 0.6113
Epoch [37/50], Loss: 0.0165, val_loss: 0.0176, val_acc: 0.6090
Epoch [38/50], Loss: 0.0163, val_loss: 0.0175, val_acc: 0.6098
Epoch [39/50], Loss: 0.0162, val_loss: 0.0173, val_acc: 0.6111
Epoch [40/50], Loss: 0.0161, val_loss: 0.0177, val_acc: 0.6088
Epoch [41/50], Loss: 0.0162, val_loss: 0.0178, val_acc: 0.6058
Epoch [42/50], Loss: 0.0162, val_loss: 0.0178, val_acc: 0.5971
Epoch [43/50], Loss: 0.0161, val_loss: 0.0167, val_acc: 0.6245
Epoch [44/50], Loss: 0.0160, val_loss: 0.0172, val_acc: 0.6182
Epoch [45/50], Loss: 0.0160, val_loss: 0.0173, val_acc: 0.6158
Epoch [46/50], Loss: 0.0159, val_loss: 0.0167, val_acc: 0.6258
Epoch [47/50], Loss: 0.0159, val_loss: 0.0171, val_acc: 0.6168
Epoch [48/50], Loss: 0.0159, val_loss: 0.0169, val_acc: 0.6338
Epoch [49/50], Loss: 0.0158, val_loss: 0.0172, val_acc: 0.6154
Epoch [50/50], Loss: 0.0157, val_loss: 0.0174, val_acc: 0.6152
test_accuracy: 60.81 %
learning time: 114.039 [sec]
2DropOuts training start ...
Epoch [1/50], Loss: 0.0344, val_loss: 0.0315, val_acc: 0.2373
Epoch [2/50], Loss: 0.0303, val_loss: 0.0270, val_acc: 0.3613
Epoch [3/50], Loss: 0.0272, val_loss: 0.0255, val_acc: 0.4174
Epoch [4/50], Loss: 0.0261, val_loss: 0.0238, val_acc: 0.4532
Epoch [5/50], Loss: 0.0252, val_loss: 0.0238, val_acc: 0.4603
Epoch [6/50], Loss: 0.0248, val_loss: 0.0226, val_acc: 0.4718
Epoch [7/50], Loss: 0.0244, val_loss: 0.0226, val_acc: 0.4953
Epoch [8/50], Loss: 0.0240, val_loss: 0.0221, val_acc: 0.5005
Epoch [9/50], Loss: 0.0238, val_loss: 0.0212, val_acc: 0.5202
Epoch [10/50], Loss: 0.0235, val_loss: 0.0211, val_acc: 0.5181
Epoch [11/50], Loss: 0.0233, val_loss: 0.0211, val_acc: 0.5215
Epoch [12/50], Loss: 0.0231, val_loss: 0.0210, val_acc: 0.5301
Epoch [13/50], Loss: 0.0229, val_loss: 0.0211, val_acc: 0.5174
Epoch [14/50], Loss: 0.0227, val_loss: 0.0209, val_acc: 0.5339
Epoch [15/50], Loss: 0.0226, val_loss: 0.0216, val_acc: 0.5206
Epoch [16/50], Loss: 0.0225, val_loss: 0.0201, val_acc: 0.5439
Epoch [17/50], Loss: 0.0224, val_loss: 0.0200, val_acc: 0.5429
Epoch [18/50], Loss: 0.0223, val_loss: 0.0196, val_acc: 0.5634
Epoch [19/50], Loss: 0.0221, val_loss: 0.0199, val_acc: 0.5496
Epoch [20/50], Loss: 0.0221, val_loss: 0.0196, val_acc: 0.5670
Epoch [21/50], Loss: 0.0218, val_loss: 0.0199, val_acc: 0.5462
Epoch [22/50], Loss: 0.0217, val_loss: 0.0195, val_acc: 0.5572
Epoch [23/50], Loss: 0.0217, val_loss: 0.0195, val_acc: 0.5585
Epoch [24/50], Loss: 0.0216, val_loss: 0.0196, val_acc: 0.5559
Epoch [25/50], Loss: 0.0215, val_loss: 0.0188, val_acc: 0.5770
Epoch [26/50], Loss: 0.0215, val_loss: 0.0196, val_acc: 0.5471
Epoch [27/50], Loss: 0.0215, val_loss: 0.0189, val_acc: 0.5830
Epoch [28/50], Loss: 0.0213, val_loss: 0.0193, val_acc: 0.5622
Epoch [29/50], Loss: 0.0211, val_loss: 0.0194, val_acc: 0.5568
Epoch [30/50], Loss: 0.0211, val_loss: 0.0192, val_acc: 0.5776
Epoch [31/50], Loss: 0.0211, val_loss: 0.0186, val_acc: 0.5758
Epoch [32/50], Loss: 0.0210, val_loss: 0.0188, val_acc: 0.5757
Epoch [33/50], Loss: 0.0210, val_loss: 0.0183, val_acc: 0.5916
Epoch [34/50], Loss: 0.0210, val_loss: 0.0185, val_acc: 0.5911
Epoch [35/50], Loss: 0.0209, val_loss: 0.0188, val_acc: 0.5761
Epoch [36/50], Loss: 0.0208, val_loss: 0.0184, val_acc: 0.5881
Epoch [37/50], Loss: 0.0208, val_loss: 0.0187, val_acc: 0.5816
Epoch [38/50], Loss: 0.0208, val_loss: 0.0191, val_acc: 0.5805
Epoch [39/50], Loss: 0.0208, val_loss: 0.0183, val_acc: 0.5839
Epoch [40/50], Loss: 0.0208, val_loss: 0.0188, val_acc: 0.5898
Epoch [41/50], Loss: 0.0207, val_loss: 0.0186, val_acc: 0.5894
Epoch [42/50], Loss: 0.0207, val_loss: 0.0185, val_acc: 0.5847
Epoch [43/50], Loss: 0.0207, val_loss: 0.0184, val_acc: 0.5906
Epoch [44/50], Loss: 0.0205, val_loss: 0.0184, val_acc: 0.5898
Epoch [45/50], Loss: 0.0205, val_loss: 0.0181, val_acc: 0.5927
Epoch [46/50], Loss: 0.0205, val_loss: 0.0179, val_acc: 0.6046
Epoch [47/50], Loss: 0.0205, val_loss: 0.0191, val_acc: 0.5877
Epoch [48/50], Loss: 0.0204, val_loss: 0.0182, val_acc: 0.5934
Epoch [49/50], Loss: 0.0205, val_loss: 0.0181, val_acc: 0.5958
Epoch [50/50], Loss: 0.0203, val_loss: 0.0182, val_acc: 0.5956
test_accuracy: 58.84 %
learning time: 114.424 [sec]

--------------------------------------------------------------

Batch Norms: Default(NoDropOut)  |  test_accuracy: 55.55 %  |  learning time: 116.382 [sec]
Batch Norms: 1DropOut  |  test_accuracy: 60.81 %  |  learning time: 114.039 [sec]
Batch Norms: 2DropOuts  |  test_accuracy: 58.84 %  |  learning time: 114.424 [sec]

                                    [A0it [06:15, ?it/s]
170500096it [06:14, 455246.48it/s]
