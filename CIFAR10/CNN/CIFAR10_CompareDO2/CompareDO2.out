0it [00:00, ?it/s]  0%|          | 0/170498071 [00:01<?, ?it/s]  0%|          | 8192/170498071 [00:01<58:45, 48354.63it/s]  0%|          | 40960/170498071 [00:01<45:35, 62310.87it/s]  0%|          | 90112/170498071 [00:01<34:52, 81419.12it/s]  0%|          | 204800/170498071 [00:01<25:40, 110542.24it/s]  0%|          | 434176/170498071 [00:01<18:35, 152494.85it/s]  1%|          | 876544/170498071 [00:02<13:18, 212475.72it/s]  1%|          | 1794048/170498071 [00:02<09:25, 298431.69it/s]  2%|▏         | 3596288/170498071 [00:02<06:36, 421190.50it/s]  4%|▍         | 6709248/170498071 [00:02<04:34, 595774.04it/s]  6%|▌         | 9789440/170498071 [00:02<03:11, 839131.51it/s]  7%|▋         | 12476416/170498071 [00:02<02:14, 1171764.98it/s]  9%|▉         | 15622144/170498071 [00:03<01:35, 1629097.17it/s] 11%|█         | 18751488/170498071 [00:03<01:07, 2241552.93it/s] 13%|█▎        | 21815296/170498071 [00:03<00:48, 3038487.50it/s] 14%|█▍        | 24633344/170498071 [00:03<00:36, 4018134.28it/s] 16%|█▋        | 27762688/170498071 [00:03<00:27, 5243618.89it/s] 18%|█▊        | 30826496/170498071 [00:03<00:20, 6651257.83it/s] 20%|█▉        | 33923072/170498071 [00:04<00:16, 8204309.99it/s] 22%|██▏       | 36986880/170498071 [00:04<00:13, 10124822.39it/s] 23%|██▎       | 39247872/170498071 [00:04<00:10, 12131933.86it/s] 24%|██▍       | 41246720/170498071 [00:04<00:10, 12522836.87it/s] 25%|██▌       | 43245568/170498071 [00:04<00:09, 13225534.78it/s] 27%|██▋       | 45694976/170498071 [00:04<00:08, 15342680.23it/s] 28%|██▊       | 47636480/170498071 [00:04<00:07, 15443347.74it/s] 29%|██▉       | 49471488/170498071 [00:04<00:08, 14960428.38it/s] 31%|███       | 52125696/170498071 [00:05<00:07, 15137594.68it/s] 32%|███▏      | 55156736/170498071 [00:05<00:07, 15788668.02it/s] 34%|███▍      | 58269696/170498071 [00:05<00:06, 16367843.32it/s] 36%|███▌      | 61399040/170498071 [00:05<00:06, 16831283.43it/s] 38%|███▊      | 64462848/170498071 [00:05<00:06, 17056923.35it/s] 40%|███▉      | 67477504/170498071 [00:06<00:05, 17174545.37it/s] 41%|████      | 70213632/170498071 [00:06<00:05, 16779393.10it/s] 43%|████▎     | 73277440/170498071 [00:06<00:05, 17049277.34it/s] 45%|████▍     | 76374016/170498071 [00:06<00:05, 18604840.03it/s] 46%|████▌     | 78290944/170498071 [00:06<00:04, 18635878.61it/s] 47%|████▋     | 80199680/170498071 [00:06<00:05, 16343876.40it/s] 48%|████▊     | 82567168/170498071 [00:06<00:05, 16831975.07it/s] 50%|█████     | 85516288/170498071 [00:07<00:04, 18140808.01it/s] 51%|█████▏    | 87400448/170498071 [00:07<00:04, 18339601.77it/s] 52%|█████▏    | 89284608/170498071 [00:07<00:04, 16325079.48it/s] 54%|█████▎    | 91332608/170498071 [00:07<00:04, 17382912.62it/s] 55%|█████▍    | 93143040/170498071 [00:07<00:04, 17409792.20it/s] 56%|█████▌    | 94937088/170498071 [00:07<00:04, 16680905.35it/s] 57%|█████▋    | 97427456/170498071 [00:07<00:04, 17640213.05it/s] 58%|█████▊    | 99237888/170498071 [00:07<00:04, 17715307.86it/s] 59%|█████▉    | 101048320/170498071 [00:07<00:04, 16779214.25it/s] 61%|██████    | 103538688/170498071 [00:08<00:03, 17767403.36it/s] 62%|██████▏   | 105357312/170498071 [00:08<00:03, 17181988.44it/s] 63%|██████▎   | 107110400/170498071 [00:08<00:03, 16890382.75it/s] 64%|██████▍   | 109404160/170498071 [00:08<00:03, 17352907.48it/s] 65%|██████▌   | 111165440/170498071 [00:08<00:03, 16367420.39it/s] 66%|██████▋   | 113057792/170498071 [00:08<00:03, 16981168.05it/s] 68%|██████▊   | 115531776/170498071 [00:08<00:03, 17882944.41it/s] 69%|██████▉   | 117350400/170498071 [00:08<00:03, 16822936.05it/s] 70%|██████▉   | 119152640/170498071 [00:08<00:03, 17056482.88it/s] 71%|███████▏  | 121741312/170498071 [00:09<00:02, 18282247.82it/s] 73%|███████▎  | 123617280/170498071 [00:09<00:02, 17028466.88it/s] 74%|███████▎  | 125378560/170498071 [00:09<00:02, 17165680.35it/s] 75%|███████▍  | 127410176/170498071 [00:09<00:02, 17995140.13it/s] 76%|███████▌  | 129245184/170498071 [00:09<00:02, 16387223.83it/s] 77%|███████▋  | 131391488/170498071 [00:09<00:02, 17376568.91it/s] 78%|███████▊  | 133373952/170498071 [00:09<00:02, 17808271.45it/s] 79%|███████▉  | 135200768/170498071 [00:09<00:02, 16267234.75it/s] 81%|████████  | 137437184/170498071 [00:09<00:01, 17474997.56it/s] 82%|████████▏ | 139583488/170498071 [00:10<00:01, 18374346.95it/s] 83%|████████▎ | 141484032/170498071 [00:10<00:01, 16601699.51it/s] 84%|████████▍ | 143499264/170498071 [00:10<00:01, 15777692.46it/s] 86%|████████▌ | 146382848/170498071 [00:10<00:01, 18252798.00it/s] 87%|████████▋ | 148398080/170498071 [00:10<00:01, 17385937.98it/s] 88%|████████▊ | 150282240/170498071 [00:10<00:01, 15997848.71it/s] 90%|████████▉ | 152690688/170498071 [00:10<00:01, 16083969.19it/s] 91%|█████████▏| 155607040/170498071 [00:10<00:00, 18553335.47it/s] 92%|█████████▏| 157663232/170498071 [00:11<00:00, 17439158.15it/s] 94%|█████████▎| 159563776/170498071 [00:11<00:00, 15731406.47it/s] 95%|█████████▍| 161914880/170498071 [00:11<00:00, 16265998.68it/s] 97%|█████████▋| 164569088/170498071 [00:11<00:00, 18401138.24it/s] 98%|█████████▊| 166576128/170498071 [00:11<00:00, 17414443.15it/s] 99%|█████████▉| 168443904/170498071 [00:11<00:00, 15991913.73it/s]Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_cifar10/cifar-10-python.tar.gz
Extracting ./data_cifar10/cifar-10-python.tar.gz to ./data_cifar10/
Files already downloaded and verified
train_dataset =  40000
valid_dataset =  10000
test_dataset =  10000
Default(NoDropOut) training start ...
170500096it [00:30, 15991913.73it/s]                               Epoch [1/50], Loss: 0.0218, val_loss: 0.0179, val_acc: 0.5956
Epoch [2/50], Loss: 0.0141, val_loss: 0.0123, val_acc: 0.7315
Epoch [3/50], Loss: 0.0110, val_loss: 0.0109, val_acc: 0.7711
Epoch [4/50], Loss: 0.0089, val_loss: 0.0118, val_acc: 0.7522
Epoch [5/50], Loss: 0.0074, val_loss: 0.0120, val_acc: 0.7652
Epoch [6/50], Loss: 0.0062, val_loss: 0.0089, val_acc: 0.8107
Epoch [7/50], Loss: 0.0052, val_loss: 0.0092, val_acc: 0.8118
Epoch [8/50], Loss: 0.0043, val_loss: 0.0094, val_acc: 0.8115
Epoch [9/50], Loss: 0.0036, val_loss: 0.0105, val_acc: 0.7963
Epoch [10/50], Loss: 0.0031, val_loss: 0.0092, val_acc: 0.8293
Epoch [11/50], Loss: 0.0027, val_loss: 0.0101, val_acc: 0.8221
Epoch [12/50], Loss: 0.0023, val_loss: 0.0124, val_acc: 0.7849
Epoch [13/50], Loss: 0.0021, val_loss: 0.0104, val_acc: 0.8131
Epoch [14/50], Loss: 0.0018, val_loss: 0.0119, val_acc: 0.8145
Epoch [15/50], Loss: 0.0017, val_loss: 0.0099, val_acc: 0.8337
Epoch [16/50], Loss: 0.0015, val_loss: 0.0107, val_acc: 0.8296
Epoch [17/50], Loss: 0.0013, val_loss: 0.0105, val_acc: 0.8282
Epoch [18/50], Loss: 0.0012, val_loss: 0.0112, val_acc: 0.8314
Epoch [19/50], Loss: 0.0011, val_loss: 0.0097, val_acc: 0.8425
Epoch [20/50], Loss: 0.0011, val_loss: 0.0114, val_acc: 0.8254
Epoch [21/50], Loss: 0.0010, val_loss: 0.0117, val_acc: 0.8230
Epoch [22/50], Loss: 0.0011, val_loss: 0.0111, val_acc: 0.8301
Epoch [23/50], Loss: 0.0010, val_loss: 0.0112, val_acc: 0.8282
Epoch [24/50], Loss: 0.0009, val_loss: 0.0100, val_acc: 0.8488
Epoch [25/50], Loss: 0.0008, val_loss: 0.0110, val_acc: 0.8333
Epoch [26/50], Loss: 0.0009, val_loss: 0.0105, val_acc: 0.8384
Epoch [27/50], Loss: 0.0007, val_loss: 0.0110, val_acc: 0.8363
Epoch [28/50], Loss: 0.0008, val_loss: 0.0124, val_acc: 0.8173
Epoch [29/50], Loss: 0.0008, val_loss: 0.0110, val_acc: 0.8324
Epoch [30/50], Loss: 0.0009, val_loss: 0.0105, val_acc: 0.8368
Epoch [31/50], Loss: 0.0007, val_loss: 0.0121, val_acc: 0.8246
Epoch [32/50], Loss: 0.0008, val_loss: 0.0122, val_acc: 0.8225
Epoch [33/50], Loss: 0.0008, val_loss: 0.0103, val_acc: 0.8432
Epoch [34/50], Loss: 0.0007, val_loss: 0.0119, val_acc: 0.8235
Epoch [35/50], Loss: 0.0009, val_loss: 0.0111, val_acc: 0.8279
Epoch [36/50], Loss: 0.0007, val_loss: 0.0116, val_acc: 0.8289
Epoch [37/50], Loss: 0.0008, val_loss: 0.0137, val_acc: 0.8001
Epoch [38/50], Loss: 0.0008, val_loss: 0.0102, val_acc: 0.8451
Epoch [39/50], Loss: 0.0007, val_loss: 0.0107, val_acc: 0.8415
Epoch [40/50], Loss: 0.0008, val_loss: 0.0136, val_acc: 0.8099
Epoch [41/50], Loss: 0.0008, val_loss: 0.0127, val_acc: 0.8131
Epoch [42/50], Loss: 0.0008, val_loss: 0.0104, val_acc: 0.8425
Epoch [43/50], Loss: 0.0007, val_loss: 0.0113, val_acc: 0.8339
Epoch [44/50], Loss: 0.0008, val_loss: 0.0117, val_acc: 0.8243
Epoch [45/50], Loss: 0.0007, val_loss: 0.0135, val_acc: 0.8102
Epoch [46/50], Loss: 0.0008, val_loss: 0.0107, val_acc: 0.8327
Epoch [47/50], Loss: 0.0007, val_loss: 0.0112, val_acc: 0.8288
Epoch [48/50], Loss: 0.0007, val_loss: 0.0121, val_acc: 0.8235
Epoch [49/50], Loss: 0.0008, val_loss: 0.0113, val_acc: 0.8399
Epoch [50/50], Loss: 0.0007, val_loss: 0.0114, val_acc: 0.8297
test_accuracy: 82.89 %
learning time: 800.994 [sec]
1DropOut training start ...
Epoch [1/50], Loss: 0.0221, val_loss: 0.0181, val_acc: 0.5977
Epoch [2/50], Loss: 0.0145, val_loss: 0.0135, val_acc: 0.7031
Epoch [3/50], Loss: 0.0113, val_loss: 0.0119, val_acc: 0.7463
Epoch [4/50], Loss: 0.0092, val_loss: 0.0106, val_acc: 0.7797
Epoch [5/50], Loss: 0.0077, val_loss: 0.0114, val_acc: 0.7747
Epoch [6/50], Loss: 0.0064, val_loss: 0.0099, val_acc: 0.7943
Epoch [7/50], Loss: 0.0055, val_loss: 0.0107, val_acc: 0.7917
Epoch [8/50], Loss: 0.0046, val_loss: 0.0097, val_acc: 0.8066
Epoch [9/50], Loss: 0.0040, val_loss: 0.0100, val_acc: 0.8099
Epoch [10/50], Loss: 0.0033, val_loss: 0.0100, val_acc: 0.8128
Epoch [11/50], Loss: 0.0028, val_loss: 0.0112, val_acc: 0.8029
Epoch [12/50], Loss: 0.0026, val_loss: 0.0111, val_acc: 0.8072
Epoch [13/50], Loss: 0.0021, val_loss: 0.0119, val_acc: 0.8096
Epoch [14/50], Loss: 0.0020, val_loss: 0.0111, val_acc: 0.8181
Epoch [15/50], Loss: 0.0018, val_loss: 0.0110, val_acc: 0.8116
Epoch [16/50], Loss: 0.0016, val_loss: 0.0113, val_acc: 0.8191
Epoch [17/50], Loss: 0.0014, val_loss: 0.0116, val_acc: 0.8175
Epoch [18/50], Loss: 0.0013, val_loss: 0.0101, val_acc: 0.8367
Epoch [19/50], Loss: 0.0012, val_loss: 0.0107, val_acc: 0.8306
Epoch [20/50], Loss: 0.0012, val_loss: 0.0114, val_acc: 0.8255
Epoch [21/50], Loss: 0.0011, val_loss: 0.0134, val_acc: 0.8078
Epoch [22/50], Loss: 0.0011, val_loss: 0.0111, val_acc: 0.8260
Epoch [23/50], Loss: 0.0010, val_loss: 0.0114, val_acc: 0.8257
Epoch [24/50], Loss: 0.0011, val_loss: 0.0110, val_acc: 0.8236
Epoch [25/50], Loss: 0.0009, val_loss: 0.0109, val_acc: 0.8390
Epoch [26/50], Loss: 0.0009, val_loss: 0.0107, val_acc: 0.8373
Epoch [27/50], Loss: 0.0010, val_loss: 0.0105, val_acc: 0.8368
Epoch [28/50], Loss: 0.0009, val_loss: 0.0106, val_acc: 0.8330
Epoch [29/50], Loss: 0.0009, val_loss: 0.0109, val_acc: 0.8262
Epoch [30/50], Loss: 0.0009, val_loss: 0.0103, val_acc: 0.8451
Epoch [31/50], Loss: 0.0009, val_loss: 0.0121, val_acc: 0.8248
Epoch [32/50], Loss: 0.0008, val_loss: 0.0104, val_acc: 0.8408
Epoch [33/50], Loss: 0.0008, val_loss: 0.0117, val_acc: 0.8243
Epoch [34/50], Loss: 0.0008, val_loss: 0.0110, val_acc: 0.8346
Epoch [35/50], Loss: 0.0008, val_loss: 0.0106, val_acc: 0.8380
Epoch [36/50], Loss: 0.0009, val_loss: 0.0104, val_acc: 0.8422
Epoch [37/50], Loss: 0.0007, val_loss: 0.0121, val_acc: 0.8251
Epoch [38/50], Loss: 0.0008, val_loss: 0.0114, val_acc: 0.8238
Epoch [39/50], Loss: 0.0008, val_loss: 0.0112, val_acc: 0.8349
Epoch [40/50], Loss: 0.0008, val_loss: 0.0107, val_acc: 0.8420
Epoch [41/50], Loss: 0.0008, val_loss: 0.0116, val_acc: 0.8303
Epoch [42/50], Loss: 0.0008, val_loss: 0.0116, val_acc: 0.8346
Epoch [43/50], Loss: 0.0007, val_loss: 0.0117, val_acc: 0.8294
Epoch [44/50], Loss: 0.0008, val_loss: 0.0109, val_acc: 0.8321
Epoch [45/50], Loss: 0.0009, val_loss: 0.0108, val_acc: 0.8354
Epoch [46/50], Loss: 0.0008, val_loss: 0.0111, val_acc: 0.8306
Epoch [47/50], Loss: 0.0008, val_loss: 0.0113, val_acc: 0.8214
Epoch [48/50], Loss: 0.0008, val_loss: 0.0108, val_acc: 0.8391
Epoch [49/50], Loss: 0.0008, val_loss: 0.0106, val_acc: 0.8438
Epoch [50/50], Loss: 0.0008, val_loss: 0.0102, val_acc: 0.8479
test_accuracy: 83.98 %
learning time: 800.965 [sec]
2DropOuts training start ...
Epoch [1/50], Loss: 0.0231, val_loss: 0.0183, val_acc: 0.5844
Epoch [2/50], Loss: 0.0157, val_loss: 0.0175, val_acc: 0.6133
Epoch [3/50], Loss: 0.0125, val_loss: 0.0123, val_acc: 0.7359
Epoch [4/50], Loss: 0.0104, val_loss: 0.0119, val_acc: 0.7601
Epoch [5/50], Loss: 0.0088, val_loss: 0.0102, val_acc: 0.7860
Epoch [6/50], Loss: 0.0075, val_loss: 0.0124, val_acc: 0.7559
Epoch [7/50], Loss: 0.0065, val_loss: 0.0118, val_acc: 0.7766
Epoch [8/50], Loss: 0.0057, val_loss: 0.0108, val_acc: 0.7945
Epoch [9/50], Loss: 0.0050, val_loss: 0.0101, val_acc: 0.8071
Epoch [10/50], Loss: 0.0044, val_loss: 0.0096, val_acc: 0.8144
Epoch [11/50], Loss: 0.0037, val_loss: 0.0099, val_acc: 0.8198
Epoch [12/50], Loss: 0.0032, val_loss: 0.0114, val_acc: 0.8018
Epoch [13/50], Loss: 0.0030, val_loss: 0.0122, val_acc: 0.8058
Epoch [14/50], Loss: 0.0025, val_loss: 0.0104, val_acc: 0.8256
Epoch [15/50], Loss: 0.0023, val_loss: 0.0157, val_acc: 0.7619
Epoch [16/50], Loss: 0.0022, val_loss: 0.0114, val_acc: 0.8270
Epoch [17/50], Loss: 0.0020, val_loss: 0.0108, val_acc: 0.8278
Epoch [18/50], Loss: 0.0018, val_loss: 0.0124, val_acc: 0.8095
Epoch [19/50], Loss: 0.0018, val_loss: 0.0120, val_acc: 0.8198
Epoch [20/50], Loss: 0.0016, val_loss: 0.0107, val_acc: 0.8282
Epoch [21/50], Loss: 0.0014, val_loss: 0.0111, val_acc: 0.8312
Epoch [22/50], Loss: 0.0013, val_loss: 0.0111, val_acc: 0.8310
Epoch [23/50], Loss: 0.0014, val_loss: 0.0127, val_acc: 0.8211
Epoch [24/50], Loss: 0.0012, val_loss: 0.0114, val_acc: 0.8247
Epoch [25/50], Loss: 0.0011, val_loss: 0.0123, val_acc: 0.8248
Epoch [26/50], Loss: 0.0011, val_loss: 0.0117, val_acc: 0.8237
Epoch [27/50], Loss: 0.0011, val_loss: 0.0124, val_acc: 0.8281
Epoch [28/50], Loss: 0.0012, val_loss: 0.0130, val_acc: 0.8263
Epoch [29/50], Loss: 0.0010, val_loss: 0.0142, val_acc: 0.8079
Epoch [30/50], Loss: 0.0011, val_loss: 0.0113, val_acc: 0.8325
Epoch [31/50], Loss: 0.0011, val_loss: 0.0134, val_acc: 0.8167
Epoch [32/50], Loss: 0.0011, val_loss: 0.0113, val_acc: 0.8291
Epoch [33/50], Loss: 0.0011, val_loss: 0.0115, val_acc: 0.8344
Epoch [34/50], Loss: 0.0011, val_loss: 0.0113, val_acc: 0.8292
Epoch [35/50], Loss: 0.0010, val_loss: 0.0113, val_acc: 0.8363
Epoch [36/50], Loss: 0.0009, val_loss: 0.0121, val_acc: 0.8369
Epoch [37/50], Loss: 0.0010, val_loss: 0.0113, val_acc: 0.8400
Epoch [38/50], Loss: 0.0010, val_loss: 0.0119, val_acc: 0.8269
Epoch [39/50], Loss: 0.0010, val_loss: 0.0111, val_acc: 0.8343
Epoch [40/50], Loss: 0.0010, val_loss: 0.0113, val_acc: 0.8387
Epoch [41/50], Loss: 0.0009, val_loss: 0.0121, val_acc: 0.8257
Epoch [42/50], Loss: 0.0010, val_loss: 0.0119, val_acc: 0.8332
Epoch [43/50], Loss: 0.0010, val_loss: 0.0137, val_acc: 0.8046
Epoch [44/50], Loss: 0.0010, val_loss: 0.0115, val_acc: 0.8318
Epoch [45/50], Loss: 0.0010, val_loss: 0.0116, val_acc: 0.8328
Epoch [46/50], Loss: 0.0010, val_loss: 0.0115, val_acc: 0.8352
Epoch [47/50], Loss: 0.0009, val_loss: 0.0110, val_acc: 0.8410
Epoch [48/50], Loss: 0.0009, val_loss: 0.0131, val_acc: 0.8101
Epoch [49/50], Loss: 0.0010, val_loss: 0.0113, val_acc: 0.8375
Epoch [50/50], Loss: 0.0009, val_loss: 0.0129, val_acc: 0.8266
test_accuracy: 81.79 %
learning time: 803.160 [sec]

--------------------------------------------------------------

Batch Norms: Default(NoDropOut)  |  test_accuracy: 82.89 %  |  learning time: 800.994 [sec]
Batch Norms: 1DropOut  |  test_accuracy: 83.98 %  |  learning time: 800.965 [sec]
Batch Norms: 2DropOuts  |  test_accuracy: 81.79 %  |  learning time: 803.160 [sec]
170500096it [40:34, 70036.82it/s]   
