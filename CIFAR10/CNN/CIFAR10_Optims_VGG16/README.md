## いくつかのOptimizerに対して、CIFAR10のデータセットとVGG16ネットワークを用いて、学習曲線や性能の比較を行った。

| Optimizer   |      test_accuracy      |  learning time [sec] |
|----------|:-------------:|------:|
| SGD |  81.94 % | 669.085 |
| MomentumSGD |  83.39 % | 689.361 |
| Adam |    83.35 %  |   728.356 |
| RMSprop | 82.78 %  |    702.136 |
| Adamax | 84.23 % |    750.477 |


- ネットワークをVGG16にすると、自作のCNN多層ネットワークよりもaccが格段に向上した。学習時間は2倍ほどになった。
- 最も学習が浅いのがSGDで、それ以外のOptimizerの正答率は軒並み83%前後。Adamaxは比較的高い。時間はAdam系は比較的大きくかかっている。(学習率を動的に変化させる計算量の多さによるものと推測)
- 最も過学習が抑えられており、かつ収束が早かったのはMomentumSGDだった。SGDが最も収束が遅い。それ以外のOpptimizerはまちまち。
